{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NIDS based on Network Flow Measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is network flow and flow measurement?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A network flow consists of all network packets that have the same flow identifier (ID). The flow ID can be extracted from the packet header and consists, and is usually defined by the 5-tuple. \n",
    "\n",
    "<hr/>\n",
    "<center>\n",
    "<img src=\"images/counting/5tuple.png\"/>\n",
    "</center>\n",
    "\n",
    "Flow measurement is a collection of flow data. Measuring the flow data is useful in a number of applications such as traffic analysis, network visibility, congestion control, heavy-hitter detection, anomaly detection, and intrusion detection.\n",
    "\n",
    "Flow-measurement based NIDS use the network flow data (flow size, flow volume, flow features, etc.) for intrusion detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we take flow size/flow volume as our parameter and we consider measuring/counting the flow size/flow volume of flow IDs to detect an anomaly and intrusion. We define flow size as the number of packets and flow bytes as the byte volume of packets. We start with flow size first and later with flow volume."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How do we measure it?\n",
    "We require a counter array to count the flow size. In software, we can make use of a library to store the flow IDs and corresponding flow sizes. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the dataset first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load dataset\n",
    "import import_ipynb\n",
    "from lib.dataset import NIDSDataset\n",
    "\n",
    "data_file = 'data/packets.npy'\n",
    "labels_file = 'data/labels.npy'\n",
    "\n",
    "dataset = NIDSDataset(data_file, labels_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 20.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c0a8a32c0a8a3db2ccc4 -> 4550\n",
      "c0a8ac346d1ca6301f90 -> 540\n",
      "346d1cc0a8ac1f90a630 -> 60\n",
      "c0a8a32ac10010509eac -> 352\n",
      "c0a8a32ac10010509eae -> 240\n",
      "c0a8a32ac1001050c1f2 -> 364\n",
      "c0a8a32ac1001050c9e4 -> 520\n",
      "c0a8a32ac1001050c9e6 -> 572\n",
      "c0a8a32ac1001050c9e7 -> 520\n",
      "c0a8a32ac1001050c9e8 -> 572\n",
      "c0a8a32ac1001050c9e9 -> 624\n",
      "c0a8a32ac1001050c9eb -> 624\n",
      "c0a8a32ac1001050c9ec -> 624\n",
      "c0a8a32ac1001050c17d -> 312\n",
      "c0a8a32ac1001050c17e -> 312\n",
      "c0a8a32ac1001050c17f -> 156\n"
     ]
    }
   ],
   "source": [
    "# INITIALISE THE COUNTERS TO ZERO\n",
    "\n",
    "wordcounter=0\n",
    "framecounter=0\n",
    "flowid = \"\"\n",
    "flowvolume = \"\"\n",
    "library = {} # Library to store the flowid and sizes. Here we are taking only flow sizes (i.e; the size of\n",
    "             # each packet is taken as 1)\n",
    "\n",
    "# loop over all datasets\n",
    "for d in dataset:\n",
    "\n",
    "    decision_is_made = 0 # decision_is_made = 1 when ethertype is not 0x0800 or packet is neither TCP nor UDP\n",
    "                         # decision_is_made = 2 when the flow ID is extracted\n",
    "    wordcounter = 0\n",
    "    flowid = \"\"\n",
    "    flowvolume = \"\" \n",
    "\n",
    "    # loop over all words\n",
    "    for word in d:\n",
    "        # stop parsing if a decission is made\n",
    "        if decision_is_made == 0:\n",
    "\n",
    "            # assert Ethertype is 0x0800 - in link layer header\n",
    "            if wordcounter == 3:\n",
    "                if(word[0] == 8) and (word[1] == 0):\n",
    "                    decision_is_made = 0\n",
    "                else:\n",
    "                    decision_is_made = 1\n",
    "                    break\n",
    "            \n",
    "            # assert proto is tcp or udp 6/17 - in network layer header\n",
    "            if wordcounter == 5:\n",
    "                if(word[3] == 6)or(word[3] == 17):\n",
    "                    decision_is_made = 0\n",
    "                else:\n",
    "                    decision_is_made = 1\n",
    "                    break\n",
    "                    \n",
    "            # examine Total length - in network layer header\n",
    "            if(wordcounter == 4):\n",
    "                flowvolume += hex(word[0])[2:] # ip len 1/2\n",
    "                flowvolume += hex(word[1])[2:]  # ip len 2/2\n",
    "                \n",
    "            # examine Source Address - in network layer header\n",
    "            if wordcounter == 6:\n",
    "                flowid += hex(word[2])[2:]  # ip SA 1/4\n",
    "                flowid += hex(word[3])[2:]  # ip SA 2/4\n",
    "\n",
    "            # examine Destination Address - in network layer header\n",
    "            if wordcounter == 7:\n",
    "                flowid += hex(word[0])[2:]  # ip SA 3/4\n",
    "                flowid += hex(word[1])[2:]  # ip SA 4/4\n",
    "                flowid += hex(word[2])[2:]  # ip DA 1/4\n",
    "                flowid += hex(word[3])[2:]  # ip DA 2/4\n",
    "\n",
    "            if wordcounter == 8:\n",
    "                flowid += hex(word[0])[2:]  # ip DA 3/4\n",
    "                flowid += hex(word[1])[2:]  # ip DA 4/4\n",
    "                flowid += hex(word[2])[2:]  # ip SPort 1/2\n",
    "                flowid += hex(word[3])[2:]  # ip SPort 2/2\n",
    "\n",
    "            # examine Destination port - in transport layer header\n",
    "            if wordcounter == 9:\n",
    "                flowid += hex(word[0])[2:]  # ip DPort 1/2\n",
    "                flowid += hex(word[1])[2:]  # ip DPort 2/2\n",
    "                decision_is_made = 2\n",
    "                break\n",
    "                # TODO here you should break out of the for loop that\n",
    "                #      iterates over the words\n",
    "        # print(word, end='')\n",
    "        wordcounter += 1\n",
    "        \n",
    "    flowvolume = (int(\"0x\"+flowvolume,16))\n",
    "    \n",
    "    if(decision_is_made==2):\n",
    "        \"\"\" WRITE CODE \"\"\"\n",
    "        #check if the flowid is in the library and\n",
    "        # if it is present increment flow size by 1, and if not present add the new \n",
    "        # flowid with a size 1. \n",
    "        if flowid in library:\n",
    "            library[flowid] += flowvolume\n",
    "        else:\n",
    "            library[flowid] = flowvolume\n",
    "\n",
    "    # end of iteration over words\n",
    "\n",
    "# end of iteration over datasets\n",
    "\n",
    "# PRINTS all the flowids along with the sizes\n",
    "for flowid in library:  \n",
    "    print(flowid, '->', library[flowid])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do any of the flows exhibits anomalous behaviour?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 20.2\n",
    "\n",
    "we set a threshold to determine which of the flowids exhibits anomalous behaviour by exceeding the allocated bandwidth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following flowids exceeds the threshold:\n",
      "c0a8a32c0a8a3db2ccc4 -> 4550\n",
      "c0a8a32ac1001050c9e9 -> 624\n",
      "c0a8a32ac1001050c9eb -> 624\n",
      "c0a8a32ac1001050c9ec -> 624\n"
     ]
    }
   ],
   "source": [
    "threshold = 600\n",
    "\n",
    "print(\"The following flowids exceeds the threshold:\")\n",
    "\"\"\" WRITE code \"\"\"\n",
    "# print all the flowids from the librabry that exceeds the threshold\n",
    "for flowid in library:\n",
    "    if(library[flowid]>threshold):\n",
    "        print(flowid, '->', library[flowid])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making use of a hashtable\n",
    "If we want to reduce the memory footprint, we can hash the flowids to locate an index to store the flowids and flowsize. Python actually can make a hashtable of it, where the flow IDs and associated flow size are stored as key-value pairs. In HDL, there are no such concept as hashtables, so we have to implement the hash tables manually.\n",
    "\n",
    "<hr/>\n",
    "<center>\n",
    "<img src=\"images/counting/hashmap.png\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to address hash collisions?\n",
    "Since we are trimming down the hashed value according to the length of the hash table, there will be collisions. We need to take measures such as chaining to avoid hash collisions in hashtables. C\n",
    "\n",
    "Chaining is simple and is like a linked list, where each index can include a separate list with many elements. And the advantage is that hash table never fills up, we can always add more elements to the chain.\n",
    "\n",
    "<hr/>\n",
    "<center>\n",
    "<img src=\"images/counting/chaining.png\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 20.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import array\n",
    "import hashlib\n",
    "\n",
    "n = 256 # length of the hashtable\n",
    "array = [None] * n\n",
    "\n",
    "\"\"\" Define functions here for the hashtable \"\"\"\n",
    "\n",
    "def _hash(key):\n",
    "    \"\"\" Md5 hash function to calculate the index\"\"\"\n",
    "    n=256\n",
    "    md5 = hashlib.md5(str(hash(key)).encode('utf-8'))\n",
    "    return int(md5.hexdigest(), 16) % n\n",
    "    \n",
    "def add_ht(key, value):\n",
    "    \"\"\"Add a value to hashtable by its key and update the contents if the cell is not empty\"\"\"\n",
    "    index = _hash(key)\n",
    "    if array[index] is not None:  # This index already contain some values.\n",
    "        # WRITE the code to Check if the flowid present in the [key,value] pair in the array[index] is equal to \n",
    "        # the incoming flowid. if equal, then add the flow size to the value.\n",
    "        # If the flowids are not equal, then we have to think of the chaining and append the new\n",
    "        # element to the list in the array[index].\n",
    "        for kvp in array[index]:  \n",
    "            if kvp[0] == flowid:  \n",
    "                kvp[1] = kvp[1]+value \n",
    "                break \n",
    "        else:     \n",
    "            array[index].append([flowid, value])  \n",
    "    else: \n",
    "        # WRITE the code here\n",
    "        # If the index is empty, creare an empty list in the array[index] and append the key-value pair to the list.\n",
    "        array[index] = []\n",
    "        array[index].append([flowid, value])\n",
    "        \n",
    "def query_ht(key):\n",
    "    \"\"\"Get a value by key\"\"\"\n",
    "    index = _hash(key)\n",
    "    if array[index] is None:\n",
    "        return 0\n",
    "    else:\n",
    "        \"\"\" WRITE code \"\"\"\n",
    "        # Loop through all key-value-pairs and find if the flowid exist. \n",
    "        # If exists then return its value. # If no return was done during loop, \n",
    "        # it means flowid does not exist and return 0\n",
    "        for kvp in array[index]:\n",
    "            if kvp[0] == key:\n",
    "                return kvp[1]\n",
    "        return 0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Now, again reading the dataset to update the hashtable. Copy and paste the code \"\"\"\n",
    "\"\"\" from the Exercise 20.1 \"\"\"\n",
    "\n",
    "wordcounter=0\n",
    "framecounter=0\n",
    "flowid = \"\"     # flow id\n",
    "flowvolume = \"\" # flow volume\n",
    "flowlist = []  # keeps a list to store flows\n",
    "\n",
    "# loop over all datasets\n",
    "for d in dataset:\n",
    "\n",
    "    decision_is_made = 0 # decision_is_made = 1 when ethertype is not 0x0800 or packet is neither TCP nor UDP\n",
    "                         # decision_is_made = 2 when the flow ID is extracted\n",
    "    wordcounter = 0\n",
    "    flowid = \"\"\n",
    "    flowvolume = \"\"\n",
    "\n",
    "    # loop over all words\n",
    "    for word in d:\n",
    "        # stop parsing if a decission is made\n",
    "        if decision_is_made == 0:\n",
    "\n",
    "            # assert Ethertype is 0x0800 - in link layer header\n",
    "            if wordcounter == 3:\n",
    "                if(word[0] == 8) and (word[1] == 0):\n",
    "                    decision_is_made = 0\n",
    "                else:\n",
    "                    decision_is_made = 1\n",
    "                    break\n",
    "            \n",
    "            # assert proto is tcp or udp 6/17 - in network layer header\n",
    "            if wordcounter == 5:\n",
    "                if(word[3] == 6)or(word[3] == 17):\n",
    "                    decision_is_made = 0\n",
    "                else:\n",
    "                    decision_is_made = 1\n",
    "                    break\n",
    "                    \n",
    "            # examine Total length - in network layer header\n",
    "            if(wordcounter == 4):\n",
    "                flowvolume += hex(word[0])[2:] # ip len 1/2\n",
    "                flowvolume += hex(word[1])[2:]  # ip len 2/2\n",
    "                \n",
    "            # examine Source Address - in network layer header\n",
    "            if wordcounter == 6:\n",
    "                flowid += hex(word[2])[2:]  # ip SA 1/4\n",
    "                flowid += hex(word[3])[2:]  # ip SA 2/4\n",
    "\n",
    "            # examine Destination Address - in network layer header\n",
    "            if wordcounter == 7:\n",
    "                flowid += hex(word[0])[2:]  # ip SA 3/4\n",
    "                flowid += hex(word[1])[2:]  # ip SA 4/4\n",
    "                flowid += hex(word[2])[2:]  # ip DA 1/4\n",
    "                flowid += hex(word[3])[2:]  # ip DA 2/4\n",
    "\n",
    "            if wordcounter == 8:\n",
    "                flowid += hex(word[0])[2:]  # ip DA 3/4\n",
    "                flowid += hex(word[1])[2:]  # ip DA 4/4\n",
    "                flowid += hex(word[2])[2:]  # ip SPort 1/2\n",
    "                flowid += hex(word[3])[2:]  # ip SPort 2/2\n",
    "\n",
    "            # examine Destination port - in transport layer header\n",
    "            if wordcounter == 9:\n",
    "                flowid += hex(word[0])[2:]  # ip DPort 1/2\n",
    "                flowid += hex(word[1])[2:]  # ip DPort 2/2\n",
    "                decision_is_made = 2\n",
    "                break\n",
    "                # TODO here you should break out of the for loop that\n",
    "                #      iterates over the words\n",
    "        # print(word, end='')\n",
    "        wordcounter += 1\n",
    "        \n",
    "    flowvolume = (int(\"0x\"+flowvolume,16))\n",
    "        \n",
    "    if flowid not in flowlist:\n",
    "        flowlist.append(flowid)\n",
    "\n",
    "    # f1 = 'c0a8ac346d1ca6301f90'\n",
    "    # f2 = '346d1cc0a8ac1f90a630'\n",
    "    # f3 = 'c0a8a32ac10010509eac'\n",
    "\n",
    "\n",
    "    \"\"\" Updating the table \"\"\"\n",
    "    # Here we are taking the actual flow volume to update\n",
    "    \"\"\"WRITE code\"\"\"\n",
    "    # add flowid and flowvolume to the hashtable by calling the function\n",
    "    add_ht(flowid,flowvolume)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking for anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c0a8a32c0a8a3db2ccc4 -> 4550\n",
      "c0a8a32ac1001050c9e9 -> 624\n",
      "c0a8a32ac1001050c9eb -> 624\n",
      "c0a8a32ac1001050c9ec -> 624\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Print the flow IDs that exceeds a threshold\"\"\"\n",
    "threshold = 600\n",
    "\"\"\"WRITE code\"\"\"\n",
    "# iterate through the flowlist and print those flowids having total volume greater than the threshold\n",
    "for flowid in flowlist:\n",
    "    volume = query_ht(flowid)\n",
    "    if(volume>threshold):\n",
    "        print(flowid, '->', volume)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is this really hardware friendly?\n",
    "\n",
    "Even though hash tables and collision resistant mechanisms such as linked lists is a nice solution, linked lists are not tailored for hardware.  Also, one-to-one mapping consumes considerable amount memory as the number of flows encountered could be in millions. \n",
    "\n",
    "There we have sketches, which are hardware friendly and at the same time has some inbuilt collision resistance mechanisms. Instead of one-to-one mapping, one flow is mapped to multiple counters and each counter is shared by multiple flows in the case of collisions. Requires less memory as we don't have to store the flowids, but of course at the cost of a possibility of overestimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CM Sketch\n",
    "\n",
    "CM sketch is represented by a 2-d array of counters with width w and depth d. Depth is the number of hash functions used.\n",
    "\n",
    "<hr/>\n",
    "<center>\n",
    "<img src=\"images/counting/CM_sketch.png\"/>\n",
    "</center>\n",
    "\n",
    "h1, h2,...,hd are independant hash values of the flowid f1. Each flowid is mapped to d counters in the sketch during update and minimum of all the d counters are the result during a query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 20.4a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import array\n",
    "import hashlib\n",
    "\"\"\" CM Sketch parameters and functions  \"\"\"    \n",
    "m = 16\n",
    "d = 4\n",
    "\n",
    "#initializes an empty 2-d array\n",
    "tables = []\n",
    "for i in range(d):\n",
    "    table = array.array(\"l\", (0 for i in range(m))) # \"l\" is the typecode indicating that the type is signed long\n",
    "    tables.append(table)\n",
    "    \n",
    "# Here, instead of using d independant hashes, \n",
    "# the output of md5 hash is split into d hash values.\n",
    "def _hash(flowid):\n",
    "    \"\"\" hash computation \"\"\"\n",
    "    m=16\n",
    "    d=4\n",
    "    md5 = hashlib.md5(str(hash(flowid)).encode('utf-8'))\n",
    "    for i in range(d):\n",
    "        md5.update(str(i).encode('utf-8'))\n",
    "        yield int(md5.hexdigest(), 16) % m # yield gives a generator object and has to be iterated to read the values.\n",
    "        \n",
    "def add_cms(flowid, value):\n",
    "    threshold = 600\n",
    "    \"\"\"Add a value to hashtable by its key and update the contents if the cell is not empty\"\"\"\n",
    "    # get the d indexed locations of the sketch\n",
    "    indices = _hash(flowid)\n",
    "    # Iterate through tables and indices and update the value stored in each indexed location\n",
    "    # Whether the location is empty or not, just add the value to the already existing value\n",
    "    for table, i in zip(tables,indices):\n",
    "        \"\"\" WRITE Code to update the value in the indexed location (1 line of code)\"\"\"\n",
    "        table[i] += value\n",
    "\n",
    "def query_cms(flowid):\n",
    "    \"\"\"Get a value by key\"\"\"\n",
    "    indices = _hash(flowid)\n",
    "    \"\"\" WRITE Code \"\"\"\n",
    "    # Iterate through tables and indices and return the minimum of the values stored in the indexed locations\n",
    "    return min(table[i] for table, i in zip(tables, indices))\n",
    "\n",
    "\n",
    "# f1 = 'c0a8ac346d1ca6301f90'\n",
    "\n",
    "\"\"\" Now, again reading the dataset to update the sketch. Copy and paste the code \"\"\"\n",
    "\"\"\" from the Exercise 20.1 \"\"\"\n",
    "\n",
    "wordcounter=0\n",
    "framecounter=0\n",
    "flowid = \"\"     # flow id\n",
    "flowvolume = \"\" # flow volume\n",
    "flowlist = []  # keeps a list to store flows\n",
    "\n",
    "# loop over all datasets\n",
    "for d in dataset:\n",
    "\n",
    "    decision_is_made = 0 # decision_is_made = 1 when ethertype is not 0x0800 or packet is neither TCP nor UDP\n",
    "                         # decision_is_made = 2 when the flow ID is extracted\n",
    "    wordcounter = 0\n",
    "    flowid = \"\"\n",
    "    flowvolume = \"\"\n",
    "\n",
    "    # loop over all words\n",
    "    for word in d:\n",
    "        # stop parsing if a decission is made\n",
    "        if decision_is_made == 0:\n",
    "\n",
    "            # assert Ethertype is 0x0800 - in link layer header\n",
    "            if wordcounter == 3:\n",
    "                if(word[0] == 8) and (word[1] == 0):\n",
    "                    decision_is_made = 0\n",
    "                else:\n",
    "                    decision_is_made = 1\n",
    "                    break\n",
    "            \n",
    "            # assert proto is tcp or udp 6/17 - in network layer header\n",
    "            if wordcounter == 5:\n",
    "                if(word[3] == 6)or(word[3] == 17):\n",
    "                    decision_is_made = 0\n",
    "                else:\n",
    "                    decision_is_made = 1\n",
    "                    break\n",
    "                    \n",
    "            # examine Total length - in network layer header\n",
    "            if(wordcounter == 4):\n",
    "                flowvolume += hex(word[0])[2:] # ip len 1/2\n",
    "                flowvolume += hex(word[1])[2:]  # ip len 2/2\n",
    "                \n",
    "            # examine Source Address - in network layer header\n",
    "            if wordcounter == 6:\n",
    "                flowid += hex(word[2])[2:]  # ip SA 1/4\n",
    "                flowid += hex(word[3])[2:]  # ip SA 2/4\n",
    "\n",
    "            # examine Destination Address - in network layer header\n",
    "            if wordcounter == 7:\n",
    "                flowid += hex(word[0])[2:]  # ip SA 3/4\n",
    "                flowid += hex(word[1])[2:]  # ip SA 4/4\n",
    "                flowid += hex(word[2])[2:]  # ip DA 1/4\n",
    "                flowid += hex(word[3])[2:]  # ip DA 2/4\n",
    "\n",
    "            if wordcounter == 8:\n",
    "                flowid += hex(word[0])[2:]  # ip DA 3/4\n",
    "                flowid += hex(word[1])[2:]  # ip DA 4/4\n",
    "                flowid += hex(word[2])[2:]  # ip SPort 1/2\n",
    "                flowid += hex(word[3])[2:]  # ip SPort 2/2\n",
    "\n",
    "            # examine Destination port - in transport layer header\n",
    "            if wordcounter == 9:\n",
    "                flowid += hex(word[0])[2:]  # ip DPort 1/2\n",
    "                flowid += hex(word[1])[2:]  # ip DPort 2/2\n",
    "                decision_is_made = 2\n",
    "                break\n",
    "                # TODO here you should break out of the for loop that\n",
    "                #      iterates over the words\n",
    "        # print(word, end='')\n",
    "        wordcounter += 1\n",
    "        \n",
    "    flowvolume = (int(\"0x\"+flowvolume,16))\n",
    "        \n",
    "    if flowid not in flowlist:\n",
    "        flowlist.append(flowid)\n",
    "\n",
    "    \"\"\" Updating the table \"\"\"\n",
    "    # Here we are taking the actual flow volume to update\n",
    "    \"\"\"WRITE code\"\"\"\n",
    "    # add flowid and flowvolume to the hashtable by calling the function\n",
    "    add_cms(flowid,flowvolume)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking for anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 20.4b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c0a8a32c0a8a3db2ccc4 -> 4550\n",
      "c0a8a32ac1001050c9e9 -> 624\n",
      "c0a8a32ac1001050c9eb -> 624\n",
      "c0a8a32ac1001050c9ec -> 624\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Print the flow IDs that exceeds a threshold\"\"\"\n",
    "threshold = 600\n",
    "\"\"\"WRITE code\"\"\"\n",
    "# iterate through the flowlist and print those flowids having total volume greater than the threshold\n",
    "for flowid in flowlist:\n",
    "    volume = query_cms(flowid)\n",
    "    if(volume>threshold):\n",
    "        print(flowid, '->', volume)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some extra info\n",
    "Checking whether the threshold is exceeded can be done during the update itself and thereby the malicious flow id can be blacklisted in real-time. \n",
    "\n",
    "The see how the collisions cause overestimation in CM Sketch, you can rerun the exercise 20.4a by changing the value of m to 16. (You have to make changes in two places - in line 4, and in _hash function.). Then Run 20.4c below to check the difference. (DO NOT run the cell 20.4b again. Keep the result there so that we can compare the results of 20.4a and 20.4b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 20.4c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c0a8a32c0a8a3db2ccc4 -> 4550\n",
      "c0a8a32ac1001050c9e9 -> 624\n",
      "c0a8a32ac1001050c9eb -> 624\n",
      "c0a8a32ac1001050c9ec -> 624\n",
      "c0a8a32ac1001050c17f -> 676\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Print the flow IDs that exceeds a threshold\"\"\"\n",
    "threshold = 600\n",
    "\"\"\"WRITE code\"\"\"\n",
    "# iterate through the flowlist and print those flowids having total volume greater than the threshold\n",
    "for flowid in flowlist:\n",
    "    volume = query_cms(flowid)\n",
    "    if(volume>threshold):\n",
    "        print(flowid, '->', volume)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that one more flowid is marked as malicious because of overestimation, even though it is not. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "<center>\n",
    "Continue with the <a href=\"01_readingframes.ipynb\">next notebook</a> in a new browser tab.<br/><br/>\n",
    "<img src=\"images/footer.png\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
